# Test: Streaming APIs for Large Files
# Demonstrates chunked processing without loading entire file into memory

Story:
    Print "=== Streaming API Tests ==="
    Print ""

    # Test 1: File.ReadLines - Stream text file in chunks
    Print "Test 1: File.ReadLines (Chunked Text Processing)"
    Print "----------------------------------------"

    # Create a test file with multiple lines
    TestContent is "Line 1
Line 2
Line 3
Line 4
Line 5
Line 6
Line 7
Line 8
Line 9
Line 10"
    File.Write("test_stream.txt", TestContent)

    # Read first 3 lines (1-based: lines 1-3)
    Batch1 is File.ReadLines("test_stream.txt", 1, 3)
    Print "Batch 1 (lines 1-3):"
    For each Line in Batch1:
        Print "  " + Line

    # Read next 3 lines (1-based: lines 4-6)
    Batch2 is File.ReadLines("test_stream.txt", 4, 3)
    Print "Batch 2 (lines 4-6):"
    For each Line in Batch2:
        Print "  " + Line

    # Read remaining lines (1-based: starting at line 7)
    Batch3 is File.ReadLines("test_stream.txt", 7, 10)
    Print "Batch 3 (lines 7+):"
    For each Line in Batch3:
        Print "  " + Line

    Print "✓ Streamed 10 lines in 3 batches (constant memory)"
    Print ""

    # Test 2: File.CountLines - Fast line counting
    Print "Test 2: File.CountLines (No Full Load)"
    Print "----------------------------------------"
    TotalLines is File.CountLines("test_stream.txt")
    Print "Total lines: " + TotalLines
    Print "✓ Counted lines without loading file"
    Print ""

    # Test 3: CSV.ReadRows - Stream CSV in chunks
    Print "Test 3: CSV.ReadRows (Chunked CSV Processing)"
    Print "----------------------------------------"

    # Create test CSV
    CsvContent is "name,age,city
Alice,30,NYC
Bob,25,LA
Charlie,35,SF
David,28,Boston
Eve,32,Seattle
Frank,29,Miami
Grace,31,Denver"
    File.Write("test_stream.csv", CsvContent)

    # Read first 3 rows (1-based: rows 1-3)
    Rows1 is CSV.ReadRows("test_stream.csv", 1, 3)
    Print "Batch 1 (rows 1-3):"
    For each Row in Rows1:
        Print "  " + Row["name"] + " (" + Row["age"] + ") from " + Row["city"]

    # Read next 3 rows (1-based: rows 4-6)
    Rows2 is CSV.ReadRows("test_stream.csv", 4, 3)
    Print "Batch 2 (rows 4-6):"
    For each Row in Rows2:
        Print "  " + Row["name"] + " (" + Row["age"] + ") from " + Row["city"]

    Print "✓ Streamed 7 rows in chunks (constant memory)"
    Print ""

    # Test 4: Simulating large file processing pattern
    Print "Test 4: Processing Pattern for Large Files"
    Print "----------------------------------------"

    StartLine is 1  # 1-based: start at line 1
    BatchSize is 3
    TotalProcessed is 0

    Print "Simulating chunked processing loop:"
    Repeat 4 times:
        Batch is File.ReadLines("test_stream.txt", StartLine, BatchSize)
        Count is Batch.Length

        If Count > 0:
            Print "  Processing batch starting at line " + StartLine + " (" + Count + " lines)"
            TotalProcessed is TotalProcessed + Count
            StartLine is StartLine + BatchSize

    Print "Total lines processed: " + TotalProcessed
    Print "✓ Pattern works for any file size"
    Print ""

    Print "=== Summary ==="
    Print "✓ File.ReadLines(path, start_line, count) - Stream text files (1-based)"
    Print "✓ File.CountLines(path) - Fast line counting"
    Print "✓ CSV.ReadRows(path, start_row, count) - Stream CSV files (1-based)"
    Print ""
    Print "Benefits:"
    Print "  - Constant memory usage (no matter file size)"
    Print "  - Can process 5GB+ files on 1GB RAM machines"
    Print "  - Progress tracking possible (offset / total)"
    Print "  - Interruptible processing (pause/resume)"
